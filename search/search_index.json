{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build to Manage - Java application monitoring and logging lab During this lab we will instrument a sample java application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the aplication code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release. Lab outline Fork and clone the Github repository with a simple java servlet and review the source code Configure logging library and add log messages to the application code Enable monitoring features of the WebSphere Liberty Profile Build the application with Apache Maven Configure and run Elastic stack in Docker Compose Configure and run Prometheus and Grafana stack in Docker Compose Build the java application container and integrate with Prometheus and ELK stack Prerequisites Install the following software on your workstation or use a provided VM. JDK 8 Apache Maven Docker Docker Compose Review the application code and run it locally Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/btm-java From now on you will work on your own fork of the application project https://github.com/ username /btm-java Clone b2m-java lab repository using: git clone https://github.ibm.com/ username /b2m-java Most of the commands in this lab should be executed from the b2m-java directory: cd b2m-java Review the application code in src/main/java/application/rsapp/checkout.java . This minimalistic application simulates a transaction with randon response time on the following URL: http://localhost:9080/rsapp/checkout About 5% of requests should return error and 500 HTTP response code. Run: mvn clean install and then mvn liberty:run-server to start the local WLP application server. Use internet browser to access http://localhost:9080/rsapp/checkout The expected output is JSON formatted: { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected }","title":"1. Introduction"},{"location":"#build-to-manage-java-application-monitoring-and-logging-lab","text":"During this lab we will instrument a sample java application for logging to use with log analytics tools like Elastic stack as well as for monitoring with Prometheus and Grafana . Instrumentation of the aplication code for monitoring and logging is part of the general concept we call Build to Manage . It specifies the practice of activities developers can do in order to provide manageability aspects as part of an application release.","title":"Build to Manage - Java application monitoring and logging lab"},{"location":"#lab-outline","text":"Fork and clone the Github repository with a simple java servlet and review the source code Configure logging library and add log messages to the application code Enable monitoring features of the WebSphere Liberty Profile Build the application with Apache Maven Configure and run Elastic stack in Docker Compose Configure and run Prometheus and Grafana stack in Docker Compose Build the java application container and integrate with Prometheus and ELK stack","title":"Lab outline"},{"location":"#prerequisites","text":"Install the following software on your workstation or use a provided VM. JDK 8 Apache Maven Docker Docker Compose","title":"Prerequisites"},{"location":"#review-the-application-code-and-run-it-locally","text":"Logon to GitHub using your user and password. Access the following repository and click Fork . https://github.com/rafal-szypulka/btm-java From now on you will work on your own fork of the application project https://github.com/ username /btm-java Clone b2m-java lab repository using: git clone https://github.ibm.com/ username /b2m-java Most of the commands in this lab should be executed from the b2m-java directory: cd b2m-java Review the application code in src/main/java/application/rsapp/checkout.java . This minimalistic application simulates a transaction with randon response time on the following URL: http://localhost:9080/rsapp/checkout About 5% of requests should return error and 500 HTTP response code. Run: mvn clean install and then mvn liberty:run-server to start the local WLP application server. Use internet browser to access http://localhost:9080/rsapp/checkout The expected output is JSON formatted: { status : RSAP0001I: Transaction OK , transactionTime : 22ms } or: { error : RSAP0010E: Severe problem detected }","title":"Review the application code and run it locally"},{"location":"ICP/","text":"Containerize the app and deploy to IBM Cloud Private Create a Docker container Use provided Dockerfile to build application container: docker build -t b2m-java . Test container locally (make sure you stopped local WLP server). docker run -d -p 9080:9080 b2m-java Access the http://localhost:9080/rsapp/checkout to verify the application is running. Now, our Java application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-java application container image has been uploaded to public Docker Hub: rszypulka/b2m-java . You can also upload it to the local ICP Container Registry. Deploy b2m-java application to the ICP cluster Review provided YAML file b2m-java-icp.yml and use it to deploy the applcation to IBM Cloud Private cluster. b2m-java deployment object will pull application image container rszypulka/b2m-java from Docker Hub. kubectl apply -f b2m-java-icp.yml Verify deployment status. $ kubectl get deploy b2m-java NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE b2m-java 1 1 1 1 4h Find the external IP of the worker node where the pod is running (look for column NODE . kubectl get pod pod_name -o wide Get the external NodePort . kubect get svc b2m-java Use the browser to access the application URL: http:// node_external_ip : external_nodeport Enable monitoring using ICP Prometheus and Grafana Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-java scrape_interval: 20s static_configs: - targets: - b2m-java.default.svc:80 labels: service: btm-java group: production Make sure the indentation is correct. Import provided Grafana dashboard ibm-open-liberty-grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics. Define kubernets liveness probe for use with built-in application health check The provided b2m-java-icp.yml deployment YAML file defines the liveness probe that use the implemented /health route. livenessProbe: httpGet: path: /health port: 3001 initialDelaySeconds: 3 periodSeconds: 10 Check the URL: http:// node_external_ip : external_nodeport /health to verify current health status. Expected output: {\"status\":\"ok\"}","title":"5. Deploy to IBM Cloud Private"},{"location":"ICP/#containerize-the-app-and-deploy-to-ibm-cloud-private","text":"","title":"Containerize the app and deploy to IBM Cloud Private"},{"location":"ICP/#create-a-docker-container","text":"Use provided Dockerfile to build application container: docker build -t b2m-java . Test container locally (make sure you stopped local WLP server). docker run -d -p 9080:9080 b2m-java Access the http://localhost:9080/rsapp/checkout to verify the application is running. Now, our Java application container can be deployed on ICP cluster. Make sure the kubectl client is configured to connect to your ICP cluster. More information here . The b2m-java application container image has been uploaded to public Docker Hub: rszypulka/b2m-java . You can also upload it to the local ICP Container Registry.","title":"Create a Docker container"},{"location":"ICP/#deploy-b2m-java-application-to-the-icp-cluster","text":"Review provided YAML file b2m-java-icp.yml and use it to deploy the applcation to IBM Cloud Private cluster. b2m-java deployment object will pull application image container rszypulka/b2m-java from Docker Hub. kubectl apply -f b2m-java-icp.yml Verify deployment status. $ kubectl get deploy b2m-java NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE b2m-java 1 1 1 1 4h Find the external IP of the worker node where the pod is running (look for column NODE . kubectl get pod pod_name -o wide Get the external NodePort . kubect get svc b2m-java Use the browser to access the application URL: http:// node_external_ip : external_nodeport","title":"Deploy b2m-java application to the ICP cluster"},{"location":"ICP/#enable-monitoring-using-icp-prometheus-and-grafana","text":"Add the following configuration in the monitoring-prometheus ConfigMap, scrape_configs: section. scrape_configs: - job_name: b2m-java scrape_interval: 20s static_configs: - targets: - b2m-java.default.svc:80 labels: service: btm-java group: production Make sure the indentation is correct. Import provided Grafana dashboard ibm-open-liberty-grafana-dashboard.json . Generate ICP application traffic using provided script: ./load_test_icp.sh application_url Use the application_url collected in previous chapter. Access the ICP Grafana console and verify it properly shows metrics.","title":"Enable monitoring using ICP Prometheus and Grafana"},{"location":"ICP/#define-kubernets-liveness-probe-for-use-with-built-in-application-health-check","text":"The provided b2m-java-icp.yml deployment YAML file defines the liveness probe that use the implemented /health route. livenessProbe: httpGet: path: /health port: 3001 initialDelaySeconds: 3 periodSeconds: 10 Check the URL: http:// node_external_ip : external_nodeport /health to verify current health status. Expected output: {\"status\":\"ok\"}","title":"Define kubernets liveness probe for use with built-in application health check"},{"location":"Prometheus-Grafana/","text":"Deploy a local Prometheus and Grafana stack with Docker Compose Step 1 is already done for the lab VM and prometheus repo is located in /root/prometheus . 1). Clone the prometheus docker compose repository from Github: git clone https://github.com/vegasbrianc/prometheus 2). Add scraping job definition to the Prometheus configuration file prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-java scrape_interval: 5s static_configs: - targets: [ xxx.xxx.xxx.xxx:9080 ] labels: service: b2m-java group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. 3). Start Prometheus Grafana stack: cd ~/prometheus docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus started via: http://localhost:9090 Set the Prometheus datasource in Grafana Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with this settings: name: prometheus type: prometheus url: http://localhost:9090 access: browser Configure dashboard Grafana Dashboard to import : ibm-open-liberty-grafana-dashboard.json Expected views: CPU Memory utilization for Libety Profile: Servlet requests volume and response time:","title":"4. Prometheus and Grafana configuration"},{"location":"Prometheus-Grafana/#deploy-a-local-prometheus-and-grafana-stack-with-docker-compose","text":"Step 1 is already done for the lab VM and prometheus repo is located in /root/prometheus . 1). Clone the prometheus docker compose repository from Github: git clone https://github.com/vegasbrianc/prometheus 2). Add scraping job definition to the Prometheus configuration file prometheus/prometheus/prometheus.yml by adding (uncommenting in the lab VM) the following code within scrape_config section: - job_name: btm-java scrape_interval: 5s static_configs: - targets: [ xxx.xxx.xxx.xxx:9080 ] labels: service: b2m-java group: production replace xxx.xxx.xxx.xxx with your own host machine's IP. 3). Start Prometheus Grafana stack: cd ~/prometheus docker-compose up -d Expected output: Creating network prometheus_back-tier with the default driver Creating network prometheus_front-tier with the default driver Creating prometheus_cadvisor_1 ... done Creating prometheus_alertmanager_1 ... done Creating prometheus_node-exporter_1 ... done Creating prometheus_prometheus_1 ... done Creating prometheus_grafana_1 ... done Verify that Prometheus started via: http://localhost:9090","title":"Deploy a local Prometheus and Grafana stack with Docker Compose"},{"location":"Prometheus-Grafana/#set-the-prometheus-datasource-in-grafana","text":"Verify the prometheus datasource configuration in Grafana. If it was not already configured, create a Grafana datasource with this settings: name: prometheus type: prometheus url: http://localhost:9090 access: browser","title":"Set the Prometheus datasource in Grafana"},{"location":"Prometheus-Grafana/#configure-dashboard","text":"Grafana Dashboard to import : ibm-open-liberty-grafana-dashboard.json Expected views: CPU Memory utilization for Libety Profile: Servlet requests volume and response time:","title":"Configure dashboard"},{"location":"about/","text":"Author Rafal Szypulka (rafal.szypulka@pl.ibm.com) Acknowledgements https://github.com/RisingStack/example-prometheus-nodejs","title":"About"},{"location":"about/#author","text":"Rafal Szypulka (rafal.szypulka@pl.ibm.com)","title":"Author"},{"location":"about/#acknowledgements","text":"https://github.com/RisingStack/example-prometheus-nodejs","title":"Acknowledgements"},{"location":"logging/","text":"A production service should have both logging and monitoring. Monitoring provides a real-time and historical view on the system and application state, and alerts you in case a situation is met. In most cases, a monitoring alert is simply a trigger for you to start an investigation. Monitoring shows the symptoms of problems. Logs provide details and state on individual transactions, so you can fully understand the cause of problems. Logs provide visibility into the behavior of a running app, they are one of the most fundamental tools for debugging and finding issues within your application. If structured correctly, logs can contain a wealth of information about a specific event. Logs can tell us not only when the event took place, but also provide us with details as to the root cause. Therefore, it is important that the log entries are readable to humans and machines. According to the 12-factor application guidelines (https://12factor.net/), logs are the stream of aggregated, time-ordered events. A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. If you deviate from these guidelines, make sure that you address the operational needs for logfiles, such as logging to local files and applying log rotation policies. Configure the logging library Java takes a customizable and extensible approach to logging. While Java provides a basic logging API through the java.util.logging package, you can easily use one or more alternative logging solutions instead. In this lab we will use java.util.logging . Example implementation of logging Look for the complete code in final/checkout.complete.java in case of problems. Add the following line at the beginning of src/main/java/application/rsapp/checkout.java (after package statement), to load the logging module: import java.util.logging.Logger ; then declare logger at the top of the class: Logger logger = Logger . getLogger ( rsapp.checkout ); When you want to emit the log entry, call the logger with appropriate log level and message: logger . info ( msg ); or logger.severe(msg); Uncomment the logger calls within src/main/java/application/rsapp/checkout.java . Look for the complete code with logging in final/checkout.complete.java in case of problems. We recommend to format logs in JSON, so it will be easily readible for log analytics software like Elastic stack. For WebSphere Libety Profile it can be defined globally via environment variables. More information here . Create a server.env file within b2m-java directory with the following contents: WLP_LOGGING_CONSOLE_FORMAT=json WLP_LOGGING_CONSOLE_SOURCE=message,accessLog,ffdc,trace WLP_LOGGING_CONSOLE_LOGLEVEL=info and add instruction to copy this file to the application container to Dockerfile : COPY server.env /config/ Review the src/main/liberty/config/server.xml file. These lines are related to logging configuration: logging traceSpecification= *=info / httpAccessLogging id= accessLogging / httpEndpoint httpPort= 9080 httpsPort= 9443 host= * id= defaultHttpEndpoint accessLoggingRef= accessLogging /httpEndpoint ``` `traceSpecification` specifies the trace level and `httpAccessLogging` enables WLP access log (disabled by default). Comapare your changes to `src/main/java/application/rsapp/checkout.java` with `final/checkout-complete.java`. Verify your `Dockerfile` with `final/Dockerfile`. Re-build the `target/rsapp.war` file with: mvn clean install ## Create a Docker image for java application Use provided `Dockerfile` to build application container: cd b2m-java docker build -t b2m-java . ## Integrate with the Elastic stack The following procedure shows how to send the application logs to the local Elastic stack running in Docker. ### Deploy a local Elastic stack with Docker Compose Steps 1-3 are already done for the lab VM and `docker-elk` repo is located in `/root/docker-elk`. 1). Clone the `docker-elk` repository from Github: cd git clone https://github.com/deviantony/docker-elk 2). Replace the Logstash configuration file `docker-elk/logstash/pipeline/logstash.conf` with the following code: input { gelf { port = 5000 } } filter { json { source = \"message\" } mutate { gsub = [ \"level\", \"info\", 6, \"level\", \"error\", 3 ] } mutate { convert = { \"level\" = \"integer\" } } } output { elasticsearch { hosts = \"elasticsearch:9200\" } stdout { codec = rubydebug } } The above will reconfigure logstash to use `gelf` (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using `gelf`. 3). Edit `docker-elk\\docker-compose.yml` and modify one line: from: - 5000:5000 to - 5000:5000\\udp The change above will tell `docker-compose` to expose `udp` port `5000` instead of default `tcp` (`gelf` protocol uses `udp`). 4). Start Elastic stack: cd ~/docker-elk docker-compose up -d Expected output: Creating network \"docker-elk_elk\" with driver \"bridge\" Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 5). Verify you can access Kibana on `http://localhost:5601` ### Start node.js application container and forward logs to Elastic stack Make sure the Start application container with this command: docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java ``` Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout and check if you can see application log records in Kibana. In the lab vm environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. You can also import Kibana configuration using provided Kibana dashboards: ibm-open-liberty-kibana5-problems-dashboard.json and ibm-open-liberty-kibana5-problems-dashboard.json Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json Simulate a couple of transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout and check the Kibana dashboard: Liberty Traffic : and Liberty problems :","title":"2. Node.js logging and Elastic stack integration"},{"location":"logging/#configure-the-logging-library","text":"Java takes a customizable and extensible approach to logging. While Java provides a basic logging API through the java.util.logging package, you can easily use one or more alternative logging solutions instead. In this lab we will use java.util.logging .","title":"Configure the logging library"},{"location":"logging/#example-implementation-of-logging","text":"Look for the complete code in final/checkout.complete.java in case of problems. Add the following line at the beginning of src/main/java/application/rsapp/checkout.java (after package statement), to load the logging module: import java.util.logging.Logger ; then declare logger at the top of the class: Logger logger = Logger . getLogger ( rsapp.checkout ); When you want to emit the log entry, call the logger with appropriate log level and message: logger . info ( msg ); or logger.severe(msg); Uncomment the logger calls within src/main/java/application/rsapp/checkout.java . Look for the complete code with logging in final/checkout.complete.java in case of problems. We recommend to format logs in JSON, so it will be easily readible for log analytics software like Elastic stack. For WebSphere Libety Profile it can be defined globally via environment variables. More information here . Create a server.env file within b2m-java directory with the following contents: WLP_LOGGING_CONSOLE_FORMAT=json WLP_LOGGING_CONSOLE_SOURCE=message,accessLog,ffdc,trace WLP_LOGGING_CONSOLE_LOGLEVEL=info and add instruction to copy this file to the application container to Dockerfile : COPY server.env /config/ Review the src/main/liberty/config/server.xml file. These lines are related to logging configuration: logging traceSpecification= *=info / httpAccessLogging id= accessLogging / httpEndpoint httpPort= 9080 httpsPort= 9443 host= * id= defaultHttpEndpoint accessLoggingRef= accessLogging /httpEndpoint ``` `traceSpecification` specifies the trace level and `httpAccessLogging` enables WLP access log (disabled by default). Comapare your changes to `src/main/java/application/rsapp/checkout.java` with `final/checkout-complete.java`. Verify your `Dockerfile` with `final/Dockerfile`. Re-build the `target/rsapp.war` file with: mvn clean install ## Create a Docker image for java application Use provided `Dockerfile` to build application container: cd b2m-java docker build -t b2m-java . ## Integrate with the Elastic stack The following procedure shows how to send the application logs to the local Elastic stack running in Docker. ### Deploy a local Elastic stack with Docker Compose Steps 1-3 are already done for the lab VM and `docker-elk` repo is located in `/root/docker-elk`. 1). Clone the `docker-elk` repository from Github: cd git clone https://github.com/deviantony/docker-elk 2). Replace the Logstash configuration file `docker-elk/logstash/pipeline/logstash.conf` with the following code: input { gelf { port = 5000 } } filter { json { source = \"message\" } mutate { gsub = [ \"level\", \"info\", 6, \"level\", \"error\", 3 ] } mutate { convert = { \"level\" = \"integer\" } } } output { elasticsearch { hosts = \"elasticsearch:9200\" } stdout { codec = rubydebug } } The above will reconfigure logstash to use `gelf` (Graylog Extended Log Format) protocol supported by Docker log driver, so we can directly stream application logs to Logstash using `gelf`. 3). Edit `docker-elk\\docker-compose.yml` and modify one line: from: - 5000:5000 to - 5000:5000\\udp The change above will tell `docker-compose` to expose `udp` port `5000` instead of default `tcp` (`gelf` protocol uses `udp`). 4). Start Elastic stack: cd ~/docker-elk docker-compose up -d Expected output: Creating network \"docker-elk_elk\" with driver \"bridge\" Creating docker-elk_elasticsearch_1 ... done Creating docker-elk_kibana_1 ... done Creating docker-elk_logstash_1 ... done 5). Verify you can access Kibana on `http://localhost:5601` ### Start node.js application container and forward logs to Elastic stack Make sure the Start application container with this command: docker run --name btm-java -d -p 9080:9080 --log-driver=gelf \\ --log-opt gelf-address=udp://localhost:5000 b2m-java ``` Simulate a couple fo transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout and check if you can see application log records in Kibana. In the lab vm environment, the Elastic stack has been preconfigured, so the example Dashboard and Visualizations should be available in Kibana out of the box. You can also import Kibana configuration using provided Kibana dashboards: ibm-open-liberty-kibana5-problems-dashboard.json and ibm-open-liberty-kibana5-problems-dashboard.json Go to Kibana: http://localhost:5601 Click on Management - Saved Objects - Import Select btm-nodejs-kibana.json Simulate a couple of transactions using Firefox or curl by accessing http://localhost:9080/rsapp/checkout and check the Kibana dashboard: Liberty Traffic : and Liberty problems :","title":"Example implementation of logging"},{"location":"monitoring-instrumentation/","text":"What to instrument - the RED method There are a number of ways to instrument the java microservice code for monitoring metrics collection. In this lab we will use the WLP provided features: mpMetrics-1.1 and monitor-1.0 These features provides a /metrics REST interface that conforms to the or MicroProfile metrics 1.1 specification. Application developers can add their own custom metrics, by using the MicroProfile metrics API, alongside the metrics provided by Liberty. More information here Enable Prometheus metrics for the WLP application Go to the directory where our b2m-java application has been cloned. cd ~/b2m-java and edit the app server configuration file src/main/liberty/config/server.xml . Uncomment these three lines: feature mpMetrics-1.1 /feature feature monitor-1.0 /feature feature mpHealth-1.0 /feature Test the application locally: mvn clean install mvn liberty:run-server Access this URL via Firefox or curl : http://localhost:9080/metrics . The output should be similar to: TYPE base:classloader_total_loaded_class_count counter # HELP base:classloader_total_loaded_class_count Displays the total number of classes that have been loaded since the Java virtual machine has started execution. base:classloader_total_loaded_class_count 7890 # TYPE base:gc_global_count counter # HELP base:gc_global_count Displays the total number of collections that have occurred. This attribute lists -1 if the collection count is undefined for this collector. base:gc_global_count 6 # TYPE base:cpu_system_load_average gauge # HELP base:cpu_system_load_average Displays the system load average for the last minute. The system load average is the sum of the number of runnable entities queued to the available processors and the number of runnable entities running on the available processors averaged over a period of time. The way in which the load average is calculated is operating system specific but is typically a damped time-dependent average. If the load average is not available, a negative value is displayed. This attribute is designed to provide a hint about the system load and may be queried frequently. The load average may be unavailable on some platform where it is expensive to implement this method. base:cpu_system_load_average 0.15 # TYPE base:thread_count counter # HELP base:thread_count Displays the current number of live threads including both daemon and non-daemon threads. base:thread_count 48 # TYPE base:classloader_current_loaded_class_count counter # HELP base:classloader_current_loaded_class_count Displays the number of classes that are currently loaded in the Java virtual machine. base:classloader_current_loaded_class_count 7890 # TYPE base:gc_scavenge_time_seconds gauge # HELP base:gc_scavenge_time_seconds Displays the approximate accumulated collection elapsed time in milliseconds. This attribute displays -1 if the collection elapsed time is undefined for this collector. The Java virtual machine implementation may use a high resolution timer to measure the elapsed time. This attribute may display the same value even if the collection count has been incremented if the collection elapsed time is very short. base:gc_scavenge_time_seconds 0.463 (...) Stop the WLP server with ctrl-c and build the docker image using provided Dockerfile : docker build -t b2m-java .","title":"3. Node.js monitoring instrumentation"},{"location":"monitoring-instrumentation/#what-to-instrument-the-red-method","text":"There are a number of ways to instrument the java microservice code for monitoring metrics collection. In this lab we will use the WLP provided features: mpMetrics-1.1 and monitor-1.0 These features provides a /metrics REST interface that conforms to the or MicroProfile metrics 1.1 specification. Application developers can add their own custom metrics, by using the MicroProfile metrics API, alongside the metrics provided by Liberty. More information here","title":"What to instrument - the RED method"},{"location":"monitoring-instrumentation/#enable-prometheus-metrics-for-the-wlp-application","text":"Go to the directory where our b2m-java application has been cloned. cd ~/b2m-java and edit the app server configuration file src/main/liberty/config/server.xml . Uncomment these three lines: feature mpMetrics-1.1 /feature feature monitor-1.0 /feature feature mpHealth-1.0 /feature Test the application locally: mvn clean install mvn liberty:run-server Access this URL via Firefox or curl : http://localhost:9080/metrics . The output should be similar to: TYPE base:classloader_total_loaded_class_count counter # HELP base:classloader_total_loaded_class_count Displays the total number of classes that have been loaded since the Java virtual machine has started execution. base:classloader_total_loaded_class_count 7890 # TYPE base:gc_global_count counter # HELP base:gc_global_count Displays the total number of collections that have occurred. This attribute lists -1 if the collection count is undefined for this collector. base:gc_global_count 6 # TYPE base:cpu_system_load_average gauge # HELP base:cpu_system_load_average Displays the system load average for the last minute. The system load average is the sum of the number of runnable entities queued to the available processors and the number of runnable entities running on the available processors averaged over a period of time. The way in which the load average is calculated is operating system specific but is typically a damped time-dependent average. If the load average is not available, a negative value is displayed. This attribute is designed to provide a hint about the system load and may be queried frequently. The load average may be unavailable on some platform where it is expensive to implement this method. base:cpu_system_load_average 0.15 # TYPE base:thread_count counter # HELP base:thread_count Displays the current number of live threads including both daemon and non-daemon threads. base:thread_count 48 # TYPE base:classloader_current_loaded_class_count counter # HELP base:classloader_current_loaded_class_count Displays the number of classes that are currently loaded in the Java virtual machine. base:classloader_current_loaded_class_count 7890 # TYPE base:gc_scavenge_time_seconds gauge # HELP base:gc_scavenge_time_seconds Displays the approximate accumulated collection elapsed time in milliseconds. This attribute displays -1 if the collection elapsed time is undefined for this collector. The Java virtual machine implementation may use a high resolution timer to measure the elapsed time. This attribute may display the same value even if the collection count has been incremented if the collection elapsed time is very short. base:gc_scavenge_time_seconds 0.463 (...) Stop the WLP server with ctrl-c and build the docker image using provided Dockerfile : docker build -t b2m-java .","title":"Enable Prometheus metrics for the WLP application"}]}